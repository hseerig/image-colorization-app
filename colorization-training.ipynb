{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "name": "notebook8030ab89a8"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 5173923,
          "sourceType": "datasetVersion",
          "datasetId": 3007402
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "bardiaardakanian_mmsample_path = kagglehub.dataset_download('bardiaardakanian/mmsample')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "DJPxXCtvT26z"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**IMAGE COLOURIZATION**"
      ],
      "metadata": {
        "id": "xKtvEV95ip4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Here is the outline of the project**"
      ],
      "metadata": {
        "id": "joeFIheni--6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.Download the dataset\n",
        "\n",
        "2.Explore & analyse the dataset\n",
        "\n",
        "3.prepare the dataset for the ML training\n",
        "\n",
        "4.Train the hardcoded & baseline models\n",
        "\n",
        "5.make the predictions\n",
        "\n",
        "6.perform feature engineering\n",
        "\n",
        "7.Train & evaluate different models\n",
        "\n",
        "8.Train on a GPU with the entire dataset\n",
        "\n",
        "10.Document & publish the project online\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jjal7Rcaj6Vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.Download the Dataset"
      ],
      "metadata": {
        "id": "4Ed5sba4lUcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####steps:\n",
        "\n",
        "\n",
        "*   install required libraries\n",
        "*   Download data from kaggle\n",
        "*   view dataset files\n",
        "*   load the training set with pandas\n",
        "*   load test set with pandas\n",
        "\n"
      ],
      "metadata": {
        "id": "_dNfzI23ls_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir=\"/kaggle/input/mmsample\""
      ],
      "metadata": {
        "id": "zePkobMnpJJb",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:27.332779Z",
          "iopub.execute_input": "2025-06-30T05:46:27.333243Z",
          "iopub.status.idle": "2025-06-30T05:46:27.339685Z",
          "shell.execute_reply.started": "2025-06-30T05:46:27.333224Z",
          "shell.execute_reply": "2025-06-30T05:46:27.338823Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###view dataset files\n",
        "Lets look at the size of the files"
      ],
      "metadata": {
        "id": "hqDK6lU8pvpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(data_dir))"
      ],
      "metadata": {
        "id": "8m-tnJh-paKX",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:27.340349Z",
          "iopub.execute_input": "2025-06-30T05:46:27.340547Z",
          "iopub.status.idle": "2025-06-30T05:46:27.359025Z",
          "shell.execute_reply.started": "2025-06-30T05:46:27.340532Z",
          "shell.execute_reply": "2025-06-30T05:46:27.358352Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###‚úÖ 1. Install & Import Libraries"
      ],
      "metadata": {
        "id": "UAe_jUkCteJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from skimage.color import rgb2lab\n"
      ],
      "metadata": {
        "id": "sL33-YMctaEk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:27.360831Z",
          "iopub.execute_input": "2025-06-30T05:46:27.361217Z",
          "iopub.status.idle": "2025-06-30T05:46:34.397862Z",
          "shell.execute_reply.started": "2025-06-30T05:46:27.361194Z",
          "shell.execute_reply": "2025-06-30T05:46:34.397249Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###‚úÖ 2. Check for GPU"
      ],
      "metadata": {
        "id": "SnblwBSht2b7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "T-PXnNy7trYS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:34.398356Z",
          "iopub.execute_input": "2025-06-30T05:46:34.398674Z",
          "iopub.status.idle": "2025-06-30T05:46:34.453975Z",
          "shell.execute_reply.started": "2025-06-30T05:46:34.398638Z",
          "shell.execute_reply": "2025-06-30T05:46:34.453112Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###‚úÖ 3. Load Image Paths"
      ],
      "metadata": {
        "id": "hctrkbuDuPSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This should point to the folder containing 'train2017' and 'val2017'\n",
        "dataset_path = data_dir\n",
        "\n",
        "# Get all image file paths from train and val folders\n",
        "image_paths = glob.glob(os.path.join(dataset_path, \"train2017\", \"*.jpg\"))\n",
        "image_paths += glob.glob(os.path.join(dataset_path, \"val2017\", \"*.jpg\"))\n",
        "\n",
        "print(f\"Total images found: {len(image_paths)}\")  # Should be 10,000\n"
      ],
      "metadata": {
        "id": "l3f8GpVXuDOg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:34.454936Z",
          "iopub.execute_input": "2025-06-30T05:46:34.455192Z",
          "iopub.status.idle": "2025-06-30T05:46:34.59791Z",
          "shell.execute_reply.started": "2025-06-30T05:46:34.455173Z",
          "shell.execute_reply": "2025-06-30T05:46:34.597364Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###‚úÖ 4. Split into Train and Validation"
      ],
      "metadata": {
        "id": "4znHSy2Yu9p3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle and pick 8000 for training, 2000 for validation\n",
        "np.random.seed(42)\n",
        "chosen_paths = np.random.choice(image_paths, 10000, replace=False)\n",
        "\n",
        "train_paths = chosen_paths[:8000]\n",
        "val_paths = chosen_paths[8000:]\n",
        "\n",
        "print(\"Train images:\", len(train_paths))\n",
        "print(\"Validation images:\", len(val_paths))\n"
      ],
      "metadata": {
        "id": "nestPw-Xu6Ru",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:34.598529Z",
          "iopub.execute_input": "2025-06-30T05:46:34.598755Z",
          "iopub.status.idle": "2025-06-30T05:46:34.608175Z",
          "shell.execute_reply.started": "2025-06-30T05:46:34.598739Z",
          "shell.execute_reply": "2025-06-30T05:46:34.60764Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###‚úÖ 5. Visualize Sample Images"
      ],
      "metadata": {
        "id": "1mJzkeygvMpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 4 training images\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(4):\n",
        "    img = Image.open(train_paths[i])\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H5crmHkDvDKV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:34.608828Z",
          "iopub.execute_input": "2025-06-30T05:46:34.609015Z",
          "iopub.status.idle": "2025-06-30T05:46:34.994437Z",
          "shell.execute_reply.started": "2025-06-30T05:46:34.608999Z",
          "shell.execute_reply": "2025-06-30T05:46:34.993775Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Prepare the dataset for the ml model"
      ],
      "metadata": {
        "id": "2K6S2HU4v1EJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###‚úÖ 1. Define Custom Dataset Class"
      ],
      "metadata": {
        "id": "9E0huP44wvsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Desired size to resize all images to\n",
        "IMAGE_SIZE = 256\n",
        "\n",
        "class ImageColorizationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for image colorization tasks.\n",
        "\n",
        "    - Loads RGB images from a list of file paths\n",
        "    - Converts them to LAB color space\n",
        "    - Normalizes L channel (input) and ab channels (target)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_list, is_train=True):\n",
        "        # Define image preprocessing and optional data augmentation\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),  # Resize to 256x256\n",
        "            transforms.RandomHorizontalFlip() if is_train else transforms.Lambda(lambda x: x)\n",
        "            # Randomly flip images only during training\n",
        "        ])\n",
        "        self.image_list = image_list      # List of file paths to images\n",
        "        self.is_train = is_train          # True if training mode, False for validation/test\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return total number of images in dataset\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the image and ensure it is in RGB mode\n",
        "        img = Image.open(self.image_list[idx]).convert(\"RGB\")\n",
        "\n",
        "        # Apply resizing and flipping transforms\n",
        "        img = self.transforms(img)\n",
        "\n",
        "        # Convert PIL Image to NumPy array (H x W x C)\n",
        "        img = np.array(img)\n",
        "\n",
        "        # Convert the RGB image to LAB color space\n",
        "        # LAB separates brightness (L) from color (a, b), helpful for colorization\n",
        "        lab_img = rgb2lab(img).astype(\"float32\")  # Shape: [H, W, 3]\n",
        "\n",
        "        # Convert to PyTorch tensor and change shape to [C, H, W]\n",
        "        lab_img = torch.from_numpy(lab_img).permute(2, 0, 1)  # Shape: [3, H, W]\n",
        "\n",
        "        # Normalize the L (lightness) channel to [-1, 1]\n",
        "        # Original range is [0, 100] ‚Üí (L / 50) - 1 maps it to [-1, 1]\n",
        "        L = lab_img[[0]] / 50.0 - 1.0  # Shape: [1, H, W]\n",
        "\n",
        "        # Normalize the ab (color) channels to [-1, 1]\n",
        "        # Original range is roughly [-110, 110] ‚Üí divide by 110\n",
        "        ab = lab_img[1:] / 110.0       # Shape: [2, H, W]\n",
        "\n",
        "        # Return both the input (L) and target (ab) as a dictionary\n",
        "        return {'L': L, 'ab': ab}\n"
      ],
      "metadata": {
        "id": "tx77jcrtw2cb",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:34.995159Z",
          "iopub.execute_input": "2025-06-30T05:46:34.995381Z",
          "iopub.status.idle": "2025-06-30T05:46:35.001964Z",
          "shell.execute_reply.started": "2025-06-30T05:46:34.995362Z",
          "shell.execute_reply": "2025-06-30T05:46:35.001246Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###‚úÖ 2. Create DataLoaders"
      ],
      "metadata": {
        "id": "eHjJmg0Tyi6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_dataloader(image_list, is_train=True, batch_size=16):\n",
        "    \"\"\"\n",
        "    Creates and returns a DataLoader for the ImageColorizationDataset.\n",
        "\n",
        "    Parameters:\n",
        "    - image_list (list): List of image file paths to load.\n",
        "    - is_train (bool): Whether the loader is for training (enables shuffling and augmentation).\n",
        "    - batch_size (int): Number of samples per batch.\n",
        "\n",
        "    Returns:\n",
        "    - DataLoader: PyTorch DataLoader object that yields batches of {'L': ..., 'ab': ...}.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the custom dataset\n",
        "    dataset = ImageColorizationDataset(image_list, is_train=is_train)\n",
        "\n",
        "    # Create and return a DataLoader with:\n",
        "    # - shuffling for training\n",
        "    # - no shuffling for validation/testing\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=is_train,      # Only shuffle during training\n",
        "        num_workers=2,         # Number of parallel data loading threads\n",
        "        pin_memory=True        # Faster data transfer to CUDA (if using GPU)\n",
        "    )\n",
        "\n",
        "    return loader\n"
      ],
      "metadata": {
        "id": "cSiaDqeNytRw",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:35.00453Z",
          "iopub.execute_input": "2025-06-30T05:46:35.005092Z",
          "iopub.status.idle": "2025-06-30T05:46:35.022054Z",
          "shell.execute_reply.started": "2025-06-30T05:46:35.005075Z",
          "shell.execute_reply": "2025-06-30T05:46:35.021378Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation DataLoaders using the helper function\n",
        "# These will load batches of L and ab values from the dataset\n",
        "train_dl = get_dataloader(image_list=train_paths, is_train=True)\n",
        "val_dl = get_dataloader(image_list=val_paths, is_train=False)\n"
      ],
      "metadata": {
        "id": "kpXvLtsSy2EP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:35.022776Z",
          "iopub.execute_input": "2025-06-30T05:46:35.022992Z",
          "iopub.status.idle": "2025-06-30T05:46:35.039964Z",
          "shell.execute_reply.started": "2025-06-30T05:46:35.022971Z",
          "shell.execute_reply": "2025-06-30T05:46:35.039278Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# üß™ Test the DataLoader by retrieving one batch from the training set\n",
        "data = next(iter(train_dl))  # Get the first batch from train DataLoader\n",
        "\n",
        "# Separate the L and ab channels from the batch\n",
        "Ls = data['L']    # Grayscale input images, shape: [batch_size, 1, 256, 256]\n",
        "abs_ = data['ab'] # Color channels, shape: [batch_size, 2, 256, 256]\n",
        "\n",
        "# Print the shape of L and ab to confirm\n",
        "print(\"L channel shape:\", Ls.shape)   # Expected: torch.Size([16, 1, 256, 256])\n",
        "print(\"ab channel shape:\", abs_.shape)  # Expected: torch.Size([16, 2, 256, 256])\n",
        "\n",
        "# Print how many batches are in each DataLoader\n",
        "print(\"Batches: train =\", len(train_dl), \", val =\", len(val_dl))\n"
      ],
      "metadata": {
        "id": "F5j6hsalzcXi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:35.040796Z",
          "iopub.execute_input": "2025-06-30T05:46:35.041027Z",
          "iopub.status.idle": "2025-06-30T05:46:35.870585Z",
          "shell.execute_reply.started": "2025-06-30T05:46:35.041008Z",
          "shell.execute_reply": "2025-06-30T05:46:35.869892Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.Unet generator"
      ],
      "metadata": {
        "id": "Le8kvZpezo1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class UNetBlock(nn.Module):\n",
        "    def __init__(self, outer_channels, inner_channels, submodule=None, input_channels=None,\n",
        "                 use_dropout=False, innermost=False, outermost=False):\n",
        "        super().__init__()\n",
        "        self.outermost = outermost\n",
        "\n",
        "        if input_channels is None:\n",
        "            input_channels = outer_channels\n",
        "\n",
        "        downconv = nn.Conv2d(input_channels, inner_channels, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        downrelu = nn.LeakyReLU(0.2, inplace=True)\n",
        "        downnorm = nn.BatchNorm2d(inner_channels)\n",
        "\n",
        "        uprelu = nn.ReLU(inplace=True)\n",
        "        upnorm = nn.BatchNorm2d(outer_channels)\n",
        "\n",
        "        if outermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_channels * 2, outer_channels, kernel_size=4, stride=2, padding=1)\n",
        "            down = [downconv]\n",
        "            up = [uprelu, upconv, nn.Tanh()]\n",
        "            model = down + [submodule] + up\n",
        "        elif innermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_channels, outer_channels, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "            down = [downrelu, downconv]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            model = down + up\n",
        "        else:\n",
        "            upconv = nn.ConvTranspose2d(inner_channels * 2, outer_channels, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "            down = [downrelu, downconv, downnorm]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            if use_dropout:\n",
        "                up.append(nn.Dropout(0.5))\n",
        "            model = down + [submodule] + up\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.outermost:\n",
        "            return self.model(x)\n",
        "        return torch.cat([x, self.model(x)], dim=1)\n"
      ],
      "metadata": {
        "id": "D9cRUCESzC4S",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:35.871538Z",
          "iopub.execute_input": "2025-06-30T05:46:35.871823Z",
          "iopub.status.idle": "2025-06-30T05:46:35.880026Z",
          "shell.execute_reply.started": "2025-06-30T05:46:35.871798Z",
          "shell.execute_reply": "2025-06-30T05:46:35.879499Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß `UNetBlock`: Recursive U-Net Encoder-Decoder Unit\n",
        "\n",
        "This class defines a **modular encoder-decoder block** with skip connections. It's the building block of the full U-Net model.\n",
        "\n",
        "#### üß† Key Features:\n",
        "- Uses **Conv2D + BatchNorm + LeakyReLU** for downsampling.\n",
        "- Uses **ConvTranspose2D + BatchNorm + ReLU** for upsampling.\n",
        "- **Recursive definition** allows nesting submodules to form the U-Net.\n",
        "- Uses **`torch.cat(...)`** to apply skip connections between encoder and decoder layers.\n",
        "\n",
        "#### üîÅ Modes:\n",
        "- `outermost=True`: Last layer, applies `Tanh` activation, no skip connection.\n",
        "- `innermost=True`: Deepest bottleneck layer, no submodule inside.\n",
        "- Intermediate: Includes `Dropout` optionally, has submodules and skip connections.\n",
        "\n",
        "#### üîÅ Skip Connections:\n",
        "Skip connections are implemented via:\n",
        "```python\n",
        "torch.cat([x, self.model(x)], dim=1)\n"
      ],
      "metadata": {
        "id": "2WDOwXpgzIOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "### ‚úÖ **üß† U-Net Generator**\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, input_channels=1, output_channels=2, num_downs=8, base_filters=64):\n",
        "        super().__init__()\n",
        "\n",
        "        # Start with innermost layer\n",
        "        unet_block = UNetBlock(base_filters * 8, base_filters * 8, innermost=True)\n",
        "\n",
        "        # Add intermediate layers with dropout\n",
        "        for _ in range(num_downs - 5):\n",
        "            unet_block = UNetBlock(base_filters * 8, base_filters * 8, submodule=unet_block, use_dropout=True)\n",
        "\n",
        "        # Gradually reduce the number of filters in shallower layers\n",
        "        filters = base_filters * 8\n",
        "        for _ in range(3):\n",
        "            unet_block = UNetBlock(filters // 2, filters, submodule=unet_block)\n",
        "            filters //= 2\n",
        "\n",
        "        # Outermost block\n",
        "        self.model = UNetBlock(output_channels, filters, input_channels=input_channels, submodule=unet_block, outermost=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "1ch29KQVzPGc",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:35.880805Z",
          "iopub.execute_input": "2025-06-30T05:46:35.881048Z",
          "iopub.status.idle": "2025-06-30T05:46:35.898098Z",
          "shell.execute_reply.started": "2025-06-30T05:46:35.881029Z",
          "shell.execute_reply": "2025-06-30T05:46:35.897434Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† `UNet`: Full U-Net Generator for Image-to-Image Tasks\n",
        "\n",
        "Constructs a **U-Net architecture** by stacking multiple `UNetBlock`s.\n",
        "\n",
        "#### üß± Parameters:\n",
        "- `input_channels`: Input image channels (e.g., 1 for grayscale).\n",
        "- `output_channels`: Output image channels (e.g., 2 for ab color components).\n",
        "- `num_downs`: Total number of downsampling steps (controls depth of network).\n",
        "- `base_filters`: Number of filters in the first layer (default: 64).\n",
        "\n",
        "#### üß¨ Architecture Flow:\n",
        "1. Starts with an **innermost bottleneck block**.\n",
        "2. Adds several **intermediate blocks with dropout**.\n",
        "3. Adds **outer blocks** with reducing filter size.\n",
        "4. Finally wraps everything inside an **outermost block** (no skip connection).\n",
        "\n",
        "#### üìù Example:\n",
        "```python\n",
        "netG = UNet(input_channels=1, output_channels=2, num_downs=8).to(device)\n",
        "output = netG(grayscale_image)\n"
      ],
      "metadata": {
        "id": "UB73e4bozSOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[for depth understanding](https://www.youtube.com/watch?v=EHuACSjijbI&list=PLyMom0n-MBroupZiLfVSZqK5asX8KfoHL&index=5)"
      ],
      "metadata": {
        "id": "BtUwv03H4cXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self, input_channels, base_filters=64, num_downs=3):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # Initial layer (no normalization)\n",
        "        layers.append(self._block(input_channels, base_filters, normalize=False))\n",
        "\n",
        "        # Downsampling layers\n",
        "        for i in range(num_downs):\n",
        "            in_filters = base_filters * (2 ** i)\n",
        "            out_filters = base_filters * (2 ** (i + 1))\n",
        "            stride = 1 if i == (num_downs - 1) else 2  # No downsampling on the last one\n",
        "            layers.append(self._block(in_filters, out_filters, stride=stride))\n",
        "\n",
        "        # Final output layer (1 channel, no activation or normalization)\n",
        "        layers.append(self._block(out_filters, 1, stride=1, normalize=False, activate=False))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1, normalize=True, activate=True):\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=not normalize)]\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm2d(out_channels))\n",
        "        if activate:\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "gOXnEI97ymto",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:35.898871Z",
          "iopub.execute_input": "2025-06-30T05:46:35.89911Z",
          "iopub.status.idle": "2025-06-30T05:46:35.91611Z",
          "shell.execute_reply.started": "2025-06-30T05:46:35.89909Z",
          "shell.execute_reply": "2025-06-30T05:46:35.915433Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîç PatchGAN Discriminator ‚Äî `PatchDiscriminator`\n",
        "\n",
        "This is a convolutional discriminator based on **PatchGAN**, which classifies overlapping patches (instead of the entire image) as real or fake.\n",
        "\n",
        "#### üìê Architecture Overview:\n",
        "- Input: An image with `input_channels` (e.g., 3 for RGB, or 1 for grayscale).\n",
        "- Output: A **feature map** (not a scalar) where each value represents whether the corresponding patch is real or fake.\n",
        "\n",
        "#### ‚öôÔ∏è Constructor Parameters:\n",
        "- `input_channels`: Number of input channels.\n",
        "- `base_filters`: Number of filters in the first layer (default: 64).\n",
        "- `num_downs`: Number of downsampling layers (default: 3).\n",
        "\n",
        "#### üß± Block Structure (`_block` function):\n",
        "- **Conv2D** with 4x4 kernel, stride (usually 2), and padding 1.\n",
        "- Optional **BatchNorm2d** (except first and last layers).\n",
        "- Optional **LeakyReLU** (disabled for last output layer).\n",
        "\n",
        "#### üß† Forward Method:\n",
        "```python\n",
        "def forward(self, x):\n",
        "    return self.model(x)\n"
      ],
      "metadata": {
        "id": "0LBkPB8Fyn_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = PatchDiscriminator(3)\n",
        "dummy_input = torch.randn(16, 3, 256, 256) # batch_size, channels, size, size\n",
        "out = discriminator(dummy_input)\n",
        "out.shape"
      ],
      "metadata": {
        "id": "bkgD3uL75HdU",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:35.916821Z",
          "iopub.execute_input": "2025-06-30T05:46:35.917574Z",
          "iopub.status.idle": "2025-06-30T05:46:37.332611Z",
          "shell.execute_reply.started": "2025-06-30T05:46:35.917559Z",
          "shell.execute_reply": "2025-06-30T05:46:37.332001Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GANLoss(nn.Module):\n",
        "    def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store the real and fake labels as buffers (so they move with model's device)\n",
        "        self.register_buffer('real_label', torch.tensor(real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(fake_label))\n",
        "\n",
        "        # Choose the type of GAN loss\n",
        "        if gan_mode == 'vanilla':\n",
        "            self.loss = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy with logits\n",
        "        elif gan_mode == 'lsgan':\n",
        "            self.loss = nn.MSELoss()  # Least Squares GAN (more stable)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"GAN mode '{gan_mode}' is not supported\")\n",
        "\n",
        "    def get_labels(self, preds, target_is_real):\n",
        "        # Return a tensor of the same shape as preds, filled with real or fake label\n",
        "        return self.real_label.expand_as(preds) if target_is_real else self.fake_label.expand_as(preds)\n",
        "\n",
        "    def forward(self, preds, target_is_real):\n",
        "        # Compute the GAN loss between predictions and target labels\n",
        "        labels = self.get_labels(preds, target_is_real)\n",
        "        return self.loss(preds, labels)\n"
      ],
      "metadata": {
        "id": "ssvi14T8yL0h",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:37.333474Z",
          "iopub.execute_input": "2025-06-30T05:46:37.333744Z",
          "iopub.status.idle": "2025-06-30T05:46:37.339889Z",
          "shell.execute_reply.started": "2025-06-30T05:46:37.333728Z",
          "shell.execute_reply": "2025-06-30T05:46:37.339204Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéØ Custom GAN Loss Class ‚Äî `GANLoss`\n",
        "\n",
        "This is a reusable loss module designed for training GANs (Generative Adversarial Networks). It supports two types of loss functions:\n",
        "\n",
        "#### üí° Supported Modes:\n",
        "- `'vanilla'`: Uses **Binary Cross Entropy with Logits** (BCEWithLogitsLoss).\n",
        "- `'lsgan'`: Uses **Least Squares Error** (MSELoss) ‚Äî typically more stable.\n",
        "\n",
        "#### ‚öôÔ∏è Key Features:\n",
        "- Automatically handles label generation for real vs. fake predictions.\n",
        "- Registers `real_label` and `fake_label` as buffers, so they're correctly moved across devices (e.g., CPU ‚Üî GPU).\n",
        "- Can be plugged directly into a GAN training loop for both generator and discriminator losses.\n",
        "\n",
        "#### üß† How It Works:\n",
        "1. **Initialization (`__init__`)**: Sets loss type and stores real/fake label values.\n",
        "2. **`get_labels(...)`**: Creates a label tensor matching prediction shape.\n",
        "3. **`forward(...)`**: Applies the selected loss function between predictions and ground-truth labels (real or fake).\n",
        "\n",
        "#### üìù Example Usage:\n",
        "```python\n",
        "loss_fn = GANLoss(gan_mode='lsgan')\n",
        "pred_fake = discriminator(fake_img)\n",
        "loss_D_fake = loss_fn(pred_fake, target_is_real=False)\n"
      ],
      "metadata": {
        "id": "QC_87f0cyNa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def init_weights(net, init_type='norm', gain=0.02):\n",
        "    \"\"\"\n",
        "    Initializes weights of a PyTorch model using the specified method.\n",
        "\n",
        "    Parameters:\n",
        "    - net (nn.Module): The model whose weights are to be initialized.\n",
        "    - init_type (str): The initialization strategy ('norm', 'xavier', or 'kaiming').\n",
        "    - gain (float): Scaling factor used by some initialization methods.\n",
        "\n",
        "    Returns:\n",
        "    - The model with initialized weights.\n",
        "    \"\"\"\n",
        "\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "\n",
        "        # For Conv layers\n",
        "        if hasattr(m, 'weight') and 'Conv' in classname:\n",
        "            if init_type == 'norm':\n",
        "                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n",
        "            elif init_type == 'xavier':\n",
        "                nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "            else:\n",
        "                raise NotImplementedError(f\"Initialization method '{init_type}' not supported.\")\n",
        "\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "        # For BatchNorm2d layers\n",
        "        elif 'BatchNorm2d' in classname:\n",
        "            nn.init.normal_(m.weight.data, 1.0, gain)\n",
        "            nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    net.apply(init_func)\n",
        "    print(f\"‚úÖ Model initialized with '{init_type}' weights.\")\n",
        "    return net\n"
      ],
      "metadata": {
        "id": "dgn07Fn3xiYV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:37.340559Z",
          "iopub.execute_input": "2025-06-30T05:46:37.340797Z",
          "iopub.status.idle": "2025-06-30T05:46:37.357488Z",
          "shell.execute_reply.started": "2025-06-30T05:46:37.340782Z",
          "shell.execute_reply": "2025-06-30T05:46:37.356866Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† `init_weights()`: Initialize Model Weights\n",
        "\n",
        "This function initializes weights of a PyTorch model layer by layer.\n",
        "\n",
        "#### üîß Supported Initialization Methods:\n",
        "- `'norm'`: Normal distribution (mean=0, std=gain)\n",
        "- `'xavier'`: Xavier (Glorot) normal initialization\n",
        "- `'kaiming'`: Kaiming He initialization\n",
        "\n",
        "#### ‚öôÔ∏è Layer-wise Logic:\n",
        "- **Conv Layers**: Initialized based on the selected method\n",
        "- **BatchNorm2d**: Weights ~ N(1.0, gain), Bias = 0\n",
        "\n",
        "#### üìù Usage:\n",
        "Call `init_weights(model)` after defining your model architecture but before training.\n"
      ],
      "metadata": {
        "id": "-Kfn8VQ1xiCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_model(model, device):\n",
        "    \"\"\"\n",
        "    Moves the model to the specified device and initializes its weights.\n",
        "\n",
        "    Parameters:\n",
        "    - model (nn.Module): The model to initialize.\n",
        "    - device (torch.device): The device (CPU or GPU) to move the model to.\n",
        "\n",
        "    Returns:\n",
        "    - Initialized model on the specified device.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    model = init_weights(model)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "uQVyUzTaxqw4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:37.358238Z",
          "iopub.execute_input": "2025-06-30T05:46:37.358476Z",
          "iopub.status.idle": "2025-06-30T05:46:37.373993Z",
          "shell.execute_reply.started": "2025-06-30T05:46:37.358447Z",
          "shell.execute_reply": "2025-06-30T05:46:37.373427Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è `init_model()`: Prepare Model for Training\n",
        "\n",
        "Combines two tasks in one:\n",
        "\n",
        "1. **Moves the model to the specified device** (CPU or GPU).\n",
        "2. **Initializes weights** using the default `'norm'` strategy.\n",
        "\n",
        "#### üîÅ Returns:\n",
        "- A ready-to-train PyTorch model, correctly initialized and placed on the chosen device.\n",
        "\n",
        "#### üìù Example:\n",
        "```python\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = build_res_unet(...)\n",
        "net = init_model(net, device)\n"
      ],
      "metadata": {
        "id": "vQgBtUi4xxuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self, net_G=None, lr_G=2e-4, lr_D=2e-4,\n",
        "                 beta1=0.5, beta2=0.999, lambda_L1=100.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.lambda_L1 = lambda_L1\n",
        "\n",
        "        # Generator\n",
        "        if net_G is None:\n",
        "            self.net_G = init_model(UNet(input_channels=1, output_channels=2), self.device)\n",
        "        else:\n",
        "            self.net_G = net_G.to(self.device)\n",
        "\n",
        "        # Discriminator\n",
        "        self.net_D = init_model(PatchDiscriminator(input_channels=3), self.device)\n",
        "\n",
        "        # Loss functions\n",
        "        self.GANcriterion = GANLoss(gan_mode='vanilla').to(self.device)\n",
        "        self.L1criterion = nn.L1Loss()\n",
        "\n",
        "        # Optimizers\n",
        "        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n",
        "        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n",
        "\n",
        "    def set_requires_grad(self, model, requires_grad=True):\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = requires_grad\n",
        "\n",
        "    def setup_input(self, data):\n",
        "        self.L = data['L'].to(self.device)\n",
        "        self.ab = data['ab'].to(self.device)\n",
        "\n",
        "    def forward(self):\n",
        "        self.fake_color = self.net_G(self.L)\n",
        "\n",
        "    def backward_D(self):\n",
        "        fake_input = torch.cat([self.L, self.fake_color.detach()], dim=1)\n",
        "        real_input = torch.cat([self.L, self.ab], dim=1)\n",
        "\n",
        "        pred_fake = self.net_D(fake_input)\n",
        "        pred_real = self.net_D(real_input)\n",
        "\n",
        "        self.loss_D_fake = self.GANcriterion(pred_fake, False)\n",
        "        self.loss_D_real = self.GANcriterion(pred_real, True)\n",
        "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
        "        self.loss_D.backward()\n",
        "\n",
        "    def backward_G(self):\n",
        "        fake_input = torch.cat([self.L, self.fake_color], dim=1)\n",
        "        pred_fake = self.net_D(fake_input)\n",
        "\n",
        "        self.loss_G_GAN = self.GANcriterion(pred_fake, True)\n",
        "        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n",
        "        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
        "        self.loss_G.backward()\n",
        "\n",
        "    def optimize(self):\n",
        "        self.forward()\n",
        "\n",
        "        # Optimize Discriminator\n",
        "        self.set_requires_grad(self.net_D, True)\n",
        "        self.opt_D.zero_grad()\n",
        "        self.backward_D()\n",
        "        self.opt_D.step()\n",
        "\n",
        "        # Optimize Generator\n",
        "        self.set_requires_grad(self.net_D, False)\n",
        "        self.opt_G.zero_grad()\n",
        "        self.backward_G()\n",
        "        self.opt_G.step()\n"
      ],
      "metadata": {
        "id": "OIsk6HpDrJ1v",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:37.374717Z",
          "iopub.execute_input": "2025-06-30T05:46:37.374963Z",
          "iopub.status.idle": "2025-06-30T05:46:37.389739Z",
          "shell.execute_reply.started": "2025-06-30T05:46:37.374941Z",
          "shell.execute_reply": "2025-06-30T05:46:37.389197Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† MainModel Class: Full GAN Training Logic\n",
        "\n",
        "This class handles everything needed to train our GAN-based image colorization model.\n",
        "\n",
        "### ‚úÖ Key Components:\n",
        "\n",
        "- **Generator (`net_G`)**: A U-Net that takes in grayscale (L channel) and predicts ab color channels.\n",
        "- **Discriminator (`net_D`)**: A PatchGAN that evaluates whether colorized images are real or fake.\n",
        "- **Loss Functions**:\n",
        "  - `GANLoss`: Measures how well the generator is fooling the discriminator.\n",
        "  - `L1Loss`: Measures how close the predicted colors are to the ground truth.\n",
        "- **Optimizers**:\n",
        "  - `opt_G` for the generator.\n",
        "  - `opt_D` for the discriminator.\n",
        "\n",
        "### üîÅ Training Steps:\n",
        "\n",
        "- `setup_input(data)`: Loads one batch of data (L and ab) to the device.\n",
        "- `forward()`: Uses the generator to predict ab from L.\n",
        "- `backward_D()`: Updates the discriminator by comparing real vs. fake colorizations.\n",
        "- `backward_G()`: Updates the generator using adversarial + L1 losses.\n",
        "- `optimize()`: Runs both backward passes and steps the optimizers.\n",
        "\n",
        "Each batch, we:\n",
        "1. Train the **discriminator** to classify real vs. fake images.\n",
        "2. Train the **generator** to fool the discriminator and produce realistic colorization.\n",
        "\n",
        "This setup follows the [Pix2Pix](https://arxiv.org/abs/1611.07004) framework and is optimized for fast convergence and stable training.\n"
      ],
      "metadata": {
        "id": "7Fz2xRpssKV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import lab2rgb\n",
        "\n",
        "# üìä Keeps track of the average of any value (used for losses)\n",
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.count, self.avg, self.sum = 0., 0., 0.\n",
        "\n",
        "    def update(self, val, count=1):\n",
        "        self.count += count\n",
        "        self.sum += val * count\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "# üß∞ Creates AverageMeters for each loss\n",
        "def create_loss_meters():\n",
        "    return {\n",
        "        'loss_D_fake': AverageMeter(),\n",
        "        'loss_D_real': AverageMeter(),\n",
        "        'loss_D': AverageMeter(),\n",
        "        'loss_G_GAN': AverageMeter(),\n",
        "        'loss_G_L1': AverageMeter(),\n",
        "        'loss_G': AverageMeter()\n",
        "    }\n",
        "\n",
        "\n",
        "# üîÑ Updates meters with current loss values from the model\n",
        "def update_losses(model, loss_meter_dict, count):\n",
        "    for loss_name, meter in loss_meter_dict.items():\n",
        "        loss = getattr(model, loss_name)\n",
        "        meter.update(loss.item(), count)\n",
        "\n",
        "\n",
        "# üîÅ Converts LAB image tensors back to RGB for visualization\n",
        "def lab_to_rgb(L, ab):\n",
        "    L = (L + 1.) * 50.       # Denormalize L to [0, 100]\n",
        "    ab = ab * 110.           # Denormalize ab to [-110, 110]\n",
        "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
        "\n",
        "    rgb_images = []\n",
        "    for img in Lab:\n",
        "        rgb_images.append(lab2rgb(img))\n",
        "    return np.stack(rgb_images, axis=0)\n",
        "\n",
        "\n",
        "# üñºÔ∏è Visualizes 5 examples: input, fake output, and ground truth\n",
        "def visualize(model, data, save=True):\n",
        "    model.net_G.eval()\n",
        "    with torch.no_grad():\n",
        "        model.setup_input(data)\n",
        "        model.forward()\n",
        "    model.net_G.train()\n",
        "\n",
        "    fake_color = model.fake_color.detach()\n",
        "    real_color = model.ab\n",
        "    L = model.L\n",
        "\n",
        "    fake_imgs = lab_to_rgb(L, fake_color)\n",
        "    real_imgs = lab_to_rgb(L, real_color)\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 8))\n",
        "    for i in range(5):\n",
        "        # Grayscale input\n",
        "        ax = plt.subplot(3, 5, i + 1)\n",
        "        ax.imshow(L[i][0].cpu(), cmap='gray')\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        # Fake colorized output\n",
        "        ax = plt.subplot(3, 5, i + 6)\n",
        "        ax.imshow(fake_imgs[i])\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        # Ground truth color image\n",
        "        ax = plt.subplot(3, 5, i + 11)\n",
        "        ax.imshow(real_imgs[i])\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if save:\n",
        "        fig.savefig(f\"colorization_{int(time.time())}.png\")\n",
        "\n",
        "\n",
        "# üìù Prints out average values of all tracked losses\n",
        "def log_results(loss_meter_dict):\n",
        "    print(\"üîî Average Losses:\")\n",
        "    for loss_name, meter in loss_meter_dict.items():\n",
        "        print(f\"{loss_name}: {meter.avg:.5f}\")\n"
      ],
      "metadata": {
        "id": "4Bn72sWAsJzU",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:37.390453Z",
          "iopub.execute_input": "2025-06-30T05:46:37.390722Z",
          "iopub.status.idle": "2025-06-30T05:46:37.411178Z",
          "shell.execute_reply.started": "2025-06-30T05:46:37.390702Z",
          "shell.execute_reply": "2025-06-30T05:46:37.410483Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìâ Loss Tracking and üñºÔ∏è Image Visualization Utilities\n",
        "\n",
        "To effectively monitor and evaluate our GAN-based image colorization model during training, we define several helper functions and classes. Here's what each one does:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 1. `AverageMeter`\n",
        "\n",
        "Tracks the **running average of any metric**, commonly used for losses.\n",
        "\n",
        "- `reset()` ‚Äì Resets all values to zero.\n",
        "- `update(val, count)` ‚Äì Updates the internal sum and average.\n",
        "- Useful for displaying average loss at the end of each epoch.\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ 2. `create_loss_meters()`\n",
        "\n",
        "Returns a dictionary of `AverageMeter`s for:\n",
        "- Discriminator losses:\n",
        "  - `loss_D_fake`: Loss for fake samples.\n",
        "  - `loss_D_real`: Loss for real samples.\n",
        "  - `loss_D`: Combined average.\n",
        "- Generator losses:\n",
        "  - `loss_G_GAN`: GAN loss (how well it fools the discriminator).\n",
        "  - `loss_G_L1`: L1 loss (difference from true image).\n",
        "  - `loss_G`: Total generator loss (GAN + L1).\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ 3. `update_losses(model, loss_meter_dict, count)`\n",
        "\n",
        "Automatically pulls loss values from the model and updates the corresponding average meters.\n",
        "\n",
        "- Used during each batch to accumulate loss statistics.\n",
        "- `model.loss_G`, `model.loss_D_fake`, etc. are accessed dynamically.\n",
        "\n",
        "---\n",
        "\n",
        "### üé® 4. `lab_to_rgb(L, ab)`\n",
        "\n",
        "Converts LAB color space tensors back to **RGB images** for visualization.\n",
        "\n",
        "- The model works in LAB format for better color separation.\n",
        "- We denormalize the `L` and `ab` channels to their original range.\n",
        "- Then convert them to RGB using `skimage.color.lab2rgb()`.\n",
        "\n",
        "---\n",
        "\n",
        "### üñºÔ∏è 5. `visualize(model, data, save=True)`\n",
        "\n",
        "Displays **side-by-side comparison** of model output and ground truth:\n",
        "\n",
        "- Top row: Grayscale input (L)\n",
        "- Middle row: Generated color image\n",
        "- Bottom row: Real color image\n",
        "\n",
        "If `save=True`, it also saves the visualization as a PNG image with a timestamp.\n",
        "\n",
        "---\n",
        "\n",
        "### üìù 6. `log_results(loss_meter_dict)`\n",
        "\n",
        "Prints out all tracked average losses (from the `AverageMeter`s) at the end of an epoch.\n",
        "\n",
        "Example output:\n"
      ],
      "metadata": {
        "id": "7JUIM9mvsqQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_dl, epochs, display_every=200, val_dl=None):\n",
        "    # Use val_dl if available for visualizing\n",
        "    val_data = next(iter(val_dl)) if val_dl is not None else None\n",
        "\n",
        "    epoch_loss_log = []  # ‚è∫Ô∏è List of dicts: one per epoch\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        loss_meter_dict = create_loss_meters()  # e.g., G_GAN, G_L1, D_real, D_fake\n",
        "        step = 0\n",
        "\n",
        "        for data in tqdm(train_dl):\n",
        "            model.setup_input(data)\n",
        "            model.optimize()\n",
        "            update_losses(model, loss_meter_dict, count=data['L'].size(0))\n",
        "\n",
        "            step += 1\n",
        "            if step % display_every == 0:\n",
        "                print(f\"\\nEpoch {epoch+1}/{epochs} | Step {step}/{len(train_dl)}\")\n",
        "                log_results(loss_meter_dict)\n",
        "                if val_data is not None:\n",
        "                    visualize(model, val_data, save=False)\n",
        "\n",
        "        # After epoch: log average loss values\n",
        "        epoch_losses = {key: meter.avg for key, meter in loss_meter_dict.items()}\n",
        "        epoch_loss_log.append(epoch_losses)\n",
        "        print(f\"\\n‚úÖ Epoch {epoch+1} completed. Average losses:\")\n",
        "        for k, v in epoch_losses.items():\n",
        "            print(f\"  {k}: {v:.4f}\")\n",
        "\n",
        "    return epoch_loss_log  # <-- for saving loss logs\n"
      ],
      "metadata": {
        "id": "IjRufyaNstpO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:37.41197Z",
          "iopub.execute_input": "2025-06-30T05:46:37.412241Z",
          "iopub.status.idle": "2025-06-30T05:46:37.432046Z",
          "shell.execute_reply.started": "2025-06-30T05:46:37.412221Z",
          "shell.execute_reply": "2025-06-30T05:46:37.431454Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Training Loop: `train_model()`\n",
        "\n",
        "This function handles the **entire GAN training process**. It runs the model for multiple epochs, logs losses, and periodically visualizes the outputs. Here's how it works:\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Parameters\n",
        "\n",
        "- `model`: The `MainModel` instance (contains Generator + Discriminator).\n",
        "- `train_dl`: PyTorch DataLoader for training data.\n",
        "- `epochs`: Number of full passes through the dataset.\n",
        "- `display_every`: How often (in steps) to show progress and visualize output.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Workflow Explained\n",
        "\n",
        "#### 1. `val_data = next(iter(val_dl))`\n",
        "\n",
        "- Takes a single batch from the validation set.\n",
        "- This batch is used repeatedly for visualizing model performance during training.\n",
        "\n",
        "#### 2. `loss_meter_dict = create_loss_meters()`\n",
        "\n",
        "- Initializes tracking meters for all loss components (`D_fake`, `D_real`, `G_GAN`, `G_L1`, etc.).\n",
        "- Tracks **average loss per epoch**.\n",
        "\n",
        "#### 3. `model.setup_input(data)`  \n",
        "- Sends current batch to GPU and splits it into L (grayscale) and ab (color channels).\n",
        "\n",
        "#### 4. `model.optimize()`\n",
        "\n",
        "- Calls:\n",
        "  - `model.forward()` ‚Üí Generates fake color\n",
        "  - `model.backward_D()` ‚Üí Updates Discriminator\n",
        "  - `model.backward_G()` ‚Üí Updates Generator\n",
        "\n",
        "#### 5. `update_losses(...)`\n",
        "\n",
        "- Updates all tracked loss meters using the latest loss values from the model.\n",
        "\n",
        "#### 6. Every `display_every` steps:\n",
        "\n",
        "- üì¢ Prints progress and current epoch/iteration.\n",
        "- üìä Calls `log_results()` to print average losses.\n",
        "- üñºÔ∏è Calls `visualize()` to show:\n",
        "  - Grayscale input\n",
        "  - Generated color image\n",
        "  - Real ground truth\n",
        "\n",
        "---\n",
        "\n",
        "This function is essential for:\n",
        "- Repeatedly improving the model through training\n",
        "- Logging the generator and discriminator performance\n",
        "- Visually monitoring how well the model is learning to col\n"
      ],
      "metadata": {
        "id": "c_YgY2aYtZCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Install FastAI if not already installed\n",
        "# !pip install fastai==2.4\n",
        "\n",
        "from fastai.vision.learner import create_body\n",
        "from fastai.vision.models.unet import DynamicUnet\n",
        "from torchvision.models.resnet import resnet18\n",
        "\n",
        "# ‚úÖ Create a U-Net Generator from a pretrained ResNet-18 encoder\n",
        "encoder = create_body(resnet18(), n_in=1, pretrained=True)\n",
        "net_G = DynamicUnet(encoder, n_out=2, img_size=(256, 256))  # ‚úÖ use n_out instead of out_channels\n"
      ],
      "metadata": {
        "id": "gIwGfl19tbuz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:37.432644Z",
          "iopub.execute_input": "2025-06-30T05:46:37.432874Z",
          "iopub.status.idle": "2025-06-30T05:46:43.545123Z",
          "shell.execute_reply.started": "2025-06-30T05:46:37.43286Z",
          "shell.execute_reply": "2025-06-30T05:46:43.544541Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Building the U-Net Generator with FastAI's DynamicUnet\n",
        "\n",
        "To simplify the creation of a powerful U-Net architecture, we use FastAI‚Äôs `DynamicUnet`, which builds a U-Net using any backbone like ResNet, EfficientNet, etc.\n",
        "\n",
        "Here‚Äôs what each line of code does:\n",
        "\n",
        "---\n",
        "\n",
        "### üîΩ 1. `from fastai.vision.learner import create_body`\n",
        "\n",
        "- Extracts the **encoder part** (feature extractor) from a pretrained model like ResNet.\n",
        "- Removes the classification head and keeps convolutional layers.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† 2. `from torchvision.models.resnet import resnet18`\n",
        "\n",
        "- Loads the **ResNet-18 architecture** from PyTorch‚Äôs model zoo.\n",
        "- We use it as the encoder (downsampling path) of our U-Net.\n",
        "- Lightweight yet effective for image colorization tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ 3. `from fastai.vision.models.unet import DynamicUnet`\n",
        "\n",
        "- `DynamicUnet` automatically builds the **decoder path** for U-Net.\n",
        "- It adds:\n",
        "  - Upsampling layers\n",
        "  - Skip connections from encoder\n",
        "  - A final convolution layer for output\n",
        "\n",
        "---\n",
        "\n",
        "### üß± 4. `create_body(resnet18(), n_in=1, pretrained=True)`\n",
        "\n",
        "- Creates a ResNet-18 backbone.\n",
        "- `n_in=1` means we‚Äôre using grayscale (L channel) images as input (1 channel).\n",
        "- `pretrained=True` loads pretrained ImageNet weights, improving performance with transfer learning.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ 5. `DynamicUnet(encoder, out_channels=2, img_size=(256, 256))`\n",
        "\n",
        "- Builds a U-Net by attaching a decoder to the encoder.\n",
        "- `out_channels=2`: We predict the a and b channels of the LAB color space.\n",
        "- `img_size=(256, 256)`: Necessary for building the correct skip connections and upsampling shapes.\n",
        "\n",
        "---\n",
        "\n",
        "üí° Using FastAI here saves you time and complexity while still leveraging powerful pretrained networks for colorization!\n"
      ],
      "metadata": {
        "id": "t5I2YVCptxiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Create U-Net Generator with Pretrained ResNet-18 Encoder (FastAI)\n",
        "\n",
        "- We use FastAI‚Äôs `DynamicUnet` to create a U-Net generator with a pretrained ResNet-18 encoder.\n",
        "- `create_body(resnet18(), n_in=1, pretrained=True)` builds the encoder with 1 input channel (grayscale).\n",
        "- `DynamicUnet(...)` wraps the encoder into a U-Net decoder:\n",
        "  - `n_out=2`: output channels (e.g., a and b in LAB color space).\n",
        "  - `img_size=(256, 256)`: target input size.\n",
        "\n",
        "> ‚ö†Ô∏è Make sure to use `n_out` instead of `out_channels`, as `DynamicUnet` does not accept `out_channels`.\n"
      ],
      "metadata": {
        "id": "ZoEyngkG06S8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from fastai.vision.learner import create_body\n",
        "from fastai.vision.models.unet import DynamicUnet\n",
        "from torchvision.models.resnet import resnet18\n",
        "\n",
        "def build_res_unet(n_input=1, n_output=2, size=256):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Create encoder (feature extractor) from a pretrained ResNet18\n",
        "    body = create_body(resnet18(pretrained=True), n_in=n_input, cut=-2)\n",
        "\n",
        "    # Attach a decoder to make a full U-Net\n",
        "    net_G = DynamicUnet(body, n_output, (size, size)).to(device)\n",
        "\n",
        "    return net_G\n"
      ],
      "metadata": {
        "id": "TvwukZ1yt0g-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:43.545915Z",
          "iopub.execute_input": "2025-06-30T05:46:43.546289Z",
          "iopub.status.idle": "2025-06-30T05:46:43.551485Z",
          "shell.execute_reply.started": "2025-06-30T05:46:43.546271Z",
          "shell.execute_reply": "2025-06-30T05:46:43.550728Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèóÔ∏è `build_res_unet`: Constructing a ResNet-based U-Net Generator\n",
        "\n",
        "This function creates a U-Net model for image colorization using FastAI and PyTorch. It builds the generator by combining a **pretrained ResNet18 encoder** with a **decoder** generated by `DynamicUnet`.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† What Each Parameter Does\n",
        "\n",
        "- `n_input`: Number of input channels (default is 1 for grayscale **L channel**).\n",
        "- `n_output`: Number of output channels (default is 2 for predicted **a and b channels** in LAB space).\n",
        "- `size`: Image size (used to define output shape and skip connections).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Step-by-Step Explanation\n",
        "\n",
        "#### 1. `device = torch.device(...)`\n",
        "\n",
        "- Automatically selects GPU (if available), otherwise uses CPU.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. `create_body(...)`\n",
        "\n",
        "- Loads the convolutional layers from a pretrained `resnet18`.\n",
        "- `n_in=n_input` lets you control the number of input channels.\n",
        "  - Set to `1` for grayscale input.\n",
        "- `cut=-2` removes the final two layers (usually average pooling and FC), leaving only the feature extractor.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. `DynamicUnet(...)`\n",
        "\n",
        "- Takes the encoder (`body`) and attaches a **decoder path** to build a complete U-Net.\n",
        "- Automatically adds skip connections and upsampling layers.\n",
        "- `n_output`: Sets the number of output channels (usually 2 for a, b channels).\n",
        "- `(size, size)`: Required to build the decoder with the correct spatial dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. `.to(device)`\n",
        "\n",
        "- Moves the model to the appropriate device (GPU or CPU).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Return\n",
        "\n",
        "- Returns a ready-to-train U-Net model (`net_G`) that takes in grayscale images and predicts color components.\n",
        "\n",
        "---\n",
        "\n",
        "üí° This function makes your generator modular and reusable ‚Äî perfect for training GANs or using separately for pretraining!\n"
      ],
      "metadata": {
        "id": "BCfcswD2uDv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "# Removed Colab-specific file upload\n",
        "import os\n",
        "\n",
        "# üìÇ Create save folder\n",
        "save_path = \"/kaggle/working/saved_model\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# ‚úÖ AverageMeter to track loss\n",
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.avg = 0\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "# ‚úÖ Training function with saving and logging\n",
        "def pretrain_generator(net_G, train_dl, opt, criterion, epochs):\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        loss_meter = AverageMeter()\n",
        "\n",
        "        for data in tqdm(train_dl):\n",
        "            L = data['L'].to(device)\n",
        "            ab = data['ab'].to(device)\n",
        "\n",
        "            preds = net_G(L)\n",
        "            loss = criterion(preds, ab)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            loss_meter.update(loss.item(), L.size(0))\n",
        "\n",
        "        avg_loss = loss_meter.avg\n",
        "        loss_history.append(avg_loss)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} - L1 Loss: {avg_loss:.5f}\")\n",
        "\n",
        "        # ‚úÖ Save checkpoint every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save(net_G.state_dict(), f\"{save_path}/checkpoint_epoch_{epoch+1}.pth\")\n",
        "\n",
        "    # ‚úÖ Save final model weights\n",
        "    torch.save(net_G.state_dict(), f\"{save_path}/res18-unet.pt\")\n",
        "    torch.save(opt.state_dict(), f\"{save_path}/opt_res18-unet.pth\")\n",
        "\n",
        "    # ‚úÖ Save loss history\n",
        "    with open(f\"{save_path}/loss_log.json\", \"w\") as f:\n",
        "        json.dump(loss_history, f)\n",
        "\n",
        "    # ‚úÖ Save config and epoch\n",
        "    config = {\n",
        "        \"architecture\": \"ResNet18-UNet\",\n",
        "        \"input_channels\": 1,\n",
        "        \"output_channels\": 2,\n",
        "        \"img_size\": [256, 256],\n",
        "        \"loss\": \"L1Loss\",\n",
        "        \"optimizer\": \"Adam\",\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"epochs_trained\": epochs\n",
        "    }\n",
        "    with open(f\"{save_path}/config.json\", \"w\") as f:\n",
        "        json.dump(config, f)\n",
        "\n",
        "    with open(f\"{save_path}/last_epoch.txt\", \"w\") as f:\n",
        "        f.write(str(epochs))\n",
        "\n",
        "    print(\"‚úÖ All files saved.\")\n",
        "\n",
        "    # üì• Download all important files\n",
        "    files.download(f\"{save_path}/res18-unet.pt\")\n",
        "    files.download(f\"{save_path}/opt_res18-unet.pth\")\n",
        "    files.download(f\"{save_path}/loss_log.json\")\n",
        "    files.download(f\"{save_path}/config.json\")\n",
        "    files.download(f\"{save_path}/last_epoch.txt\")\n"
      ],
      "metadata": {
        "id": "5pzwQyvzuH5m",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:43.552291Z",
          "iopub.execute_input": "2025-06-30T05:46:43.552517Z",
          "iopub.status.idle": "2025-06-30T05:46:43.570981Z",
          "shell.execute_reply.started": "2025-06-30T05:46:43.552493Z",
          "shell.execute_reply": "2025-06-30T05:46:43.570254Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Pretraining the Generator with L1 Loss (Before GAN Training)\n",
        "\n",
        "Before using the generator (`net_G`) in a GAN setup, we pretrain it using **only L1 loss** (also called pixel-wise loss). This helps the model learn **basic colorization** before introducing adversarial training with the discriminator.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Objective\n",
        "\n",
        "- Minimize the **difference between predicted (fake) and true (real)** ab channels.\n",
        "- Use **L1 loss** for better sharpness and stable convergence.\n",
        "- Avoids the GAN from starting from random weights.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Function: `pretrain_generator(...)`\n",
        "\n",
        "#### Inputs:\n",
        "- `net_G`: The U-Net generator (ResNet18 + decoder).\n",
        "- `train_dl`: Dataloader for training images.\n",
        "- `opt`: Optimizer (Adam in this case).\n",
        "- `criterion`: Loss function (L1).\n",
        "- `epochs`: How many full passes over the dataset.\n",
        "\n",
        "#### Steps:\n",
        "1. Loop over each epoch.\n",
        "2. For each batch:\n",
        "   - Move L and ab channels to GPU/CPU.\n",
        "   - Get predictions from the generator.\n",
        "   - Compute L1 loss: `loss = criterion(preds, ab)`\n",
        "   - Backpropagate and update weights using Adam.\n",
        "   - Update the running average loss using `AverageMeter`.\n",
        "\n",
        "#### Output:\n",
        "Prints the average L1 loss at the end of each epoch.\n",
        "\n",
        "---\n",
        "\n",
        "### üíæ Saving the Pretrained Model\n",
        "\n",
        "```python\n",
        "torch.save(net_G.state_dict(), \"/kaggle/working/res18-unet.pt\")\n"
      ],
      "metadata": {
        "id": "XtfDH7zOuX8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Build and Load the Generator (U-Net based on ResNet-18)\n",
        "\n",
        "- `build_res_unet(...)` creates a U-Net model using ResNet-18 as the encoder backbone.\n",
        "  - `n_input=1`: grayscale image input (L channel in LAB color space).\n",
        "  - `n_output=2`: outputs two channels (a and b from LAB color space).\n",
        "  - `size=256`: input image size is 256x256.\n",
        "\n",
        "- The pretrained weights are loaded from `res18-unet.pt` using `load_state_dict(...)`.\n",
        "- `map_location=device` ensures the model loads correctly on the current hardware (CPU or GPU).\n"
      ],
      "metadata": {
        "id": "DmPlSvApv9uD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß© Wrap Generator into MainModel\n",
        "\n",
        "- `MainModel` is a custom training wrapper (likely includes loss functions, optimizers, etc.).\n",
        "- We pass the prebuilt U-Net generator (`net_G`) to it.\n",
        "- This abstraction helps us train, validate, and test using one interface.\n"
      ],
      "metadata": {
        "id": "NzA3Tq2-wKmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéØ Train the Model for 20 Epochs\n",
        "\n",
        "- `train_model(...)` begins training the model.\n",
        "  - `model`: instance of `MainModel` containing the generator.\n",
        "  - `train_dl`: the DataLoader providing batches of training data.\n",
        "  - `20`: the number of training epochs.\n",
        "- This step will optimize the model weights to colorize grayscale images.\n"
      ],
      "metadata": {
        "id": "jGWg55O5wPfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üíæ Save Pretrained ResNet-UNet Generator (Optional)\n",
        "\n",
        "- This step saves the pretrained generator weights to a file (`res18-unet.pt`) for reuse later.\n",
        "- It also optionally saves the optimizer state (`opt_res18-unet.pth`), which is helpful for resuming training.\n",
        "- Use this step if you‚Äôre training the generator separately before integrating it into the full GAN model.\n"
      ],
      "metadata": {
        "id": "gRLDtiDiwuVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Save Final Trained Generator and Discriminator\n",
        "\n",
        "- After full GAN training, save the final state of both models:\n",
        "  - `final_generator.pth` ‚Äî trained U-Net generator.\n",
        "  - `final_discriminator.pth` ‚Äî trained PatchGAN discriminator.\n",
        "- This lets us use the trained models later for inference or fine-tuning.\n"
      ],
      "metadata": {
        "id": "SVnb_0M6w1Ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üí° Save Full MainModel (Convenience)\n",
        "\n",
        "- Saves the full `MainModel` wrapper's state dict (`final_model.pth`), which includes:\n",
        "  - Generator\n",
        "  - Discriminator\n",
        "  - Loss functions\n",
        "  - Any additional internal state\n",
        "- Convenient when restoring the full training setup directly without separately loading G & D.\n"
      ],
      "metadata": {
        "id": "jU-TBsrlw9QH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_G = build_res_unet(n_input=1, n_output=2, size=256).to(device)\n",
        "opt = optim.Adam(net_G.parameters(), lr=1e-4)\n",
        "criterion = nn.L1Loss()\n",
        "pretrain_generator(net_G, train_dl, opt, criterion, epochs=20)\n"
      ],
      "metadata": {
        "id": "kn2UkT4l9Q6p",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T05:46:43.571709Z",
          "iopub.execute_input": "2025-06-30T05:46:43.571916Z",
          "iopub.status.idle": "2025-06-30T06:41:02.573016Z",
          "shell.execute_reply.started": "2025-06-30T05:46:43.571902Z",
          "shell.execute_reply": "2025-06-30T06:41:02.571619Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "# Removed Colab-specific file upload\n",
        "import os\n",
        "\n",
        "# üìÇ Save folder\n",
        "gan_save_path = \"/kaggle/working/saved_gan\"\n",
        "os.makedirs(gan_save_path, exist_ok=True)\n",
        "\n",
        "# ‚úÖ Load pretrained generator\n",
        "net_G = build_res_unet(n_input=1, n_output=2, size=256).to(device)\n",
        "net_G.load_state_dict(torch.load(\"/kaggle/working/saved_model/res18-unet.pt\", map_location=device))\n",
        "\n",
        "# ‚úÖ Build MainModel and train\n",
        "model = MainModel(net_G=net_G)\n",
        "gan_loss_log = train_model(model, train_dl, epochs=20, val_dl=val_dl)\n",
        "\n",
        "# Save this log as a JSON file\n",
        "with open(\"/kaggle/working/saved_gan/gan_loss_log.json\", \"w\") as f:\n",
        "    json.dump(gan_loss_log, f)\n",
        "\n",
        "# ‚úÖ Save final GAN weights\n",
        "torch.save(model.net_G.state_dict(), f\"{gan_save_path}/final_generator.pth\")\n",
        "torch.save(model.net_D.state_dict(), f\"{gan_save_path}/final_discriminator.pth\")\n",
        "torch.save(model.state_dict(), f\"{gan_save_path}/final_model.pth\")  # optional\n",
        "\n",
        "# ‚úÖ Save GAN loss history\n",
        "with open(f\"{gan_save_path}/gan_loss_log.json\", \"w\") as f:\n",
        "    json.dump(gan_loss_log, f)\n",
        "\n",
        "# ‚úÖ Save GAN config\n",
        "gan_config = {\n",
        "    \"type\": \"Conditional GAN\",\n",
        "    \"epochs_trained\": 20,\n",
        "    \"losses\": [\"GANLoss\", \"L1Loss\"],\n",
        "    \"generator\": \"ResNet18-UNet\",\n",
        "    \"discriminator\": \"PatchGAN\"\n",
        "}\n",
        "with open(f\"{gan_save_path}/gan_config.json\", \"w\") as f:\n",
        "    json.dump(gan_config, f)\n",
        "\n",
        "# ‚úÖ Save epoch info\n",
        "with open(f\"{gan_save_path}/gan_last_epoch.txt\", \"w\") as f:\n",
        "    f.write(\"20\")\n",
        "\n",
        "# üì• Download everything\n",
        "files.download(f\"{gan_save_path}/final_generator.pth\")\n",
        "files.download(f\"{gan_save_path}/final_discriminator.pth\")\n",
        "files.download(f\"{gan_save_path}/final_model.pth\")\n",
        "files.download(f\"{gan_save_path}/gan_loss_log.json\")\n",
        "files.download(f\"{gan_save_path}/gan_config.json\")\n",
        "files.download(f\"{gan_save_path}/gan_last_epoch.txt\")\n"
      ],
      "metadata": {
        "id": "3uNVAtU77Ww5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T06:47:50.875718Z",
          "iopub.execute_input": "2025-06-30T06:47:50.876435Z",
          "iopub.status.idle": "2025-06-30T08:08:02.768736Z",
          "shell.execute_reply.started": "2025-06-30T06:47:50.876397Z",
          "shell.execute_reply": "2025-06-30T08:08:02.767668Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"‚úÖ All model weights saved to /kaggle/working/\")\n"
      ],
      "metadata": {
        "id": "9AYJH20rxBjj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-30T06:41:02.57535Z",
          "iopub.status.idle": "2025-06-30T06:41:02.575765Z",
          "shell.execute_reply.started": "2025-06-30T06:41:02.575568Z",
          "shell.execute_reply": "2025-06-30T06:41:02.575582Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pLuv-FFN0PbX",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
